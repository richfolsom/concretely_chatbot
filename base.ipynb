{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos:\n",
    "* remove the hardcoded values\n",
    "* see if I can batch the training, so I don't have to choose between a subset of data or a memory error*\n",
    "* add the ability to input a text sentence and get back a response\n",
    "* save the model to an h5 file for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation using word level language model and embeddings in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('output1.txt', names=['inp', 'outp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = lines[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7840, 2)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inp</th>\n",
       "      <th>outp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>AtAmazonHelp I cancelled my prime account 3 ti...</td>\n",
       "      <td>AtCustomerName My apologies for the unexpected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>@UPSHelp AtAmazonHelp I AM NOT PICKING THIS PA...</td>\n",
       "      <td>AtCustomerName I'm sorry to hear about this ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>AtAmazonHelp Can you please conclude so that I...</td>\n",
       "      <td>AtCustomerName I get you're upset regarding yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7090</th>\n",
       "      <td>AtAmazonHelp December the 3rd it still shows a...</td>\n",
       "      <td>AtCustomerName I'm very sorry for the inconven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>Met an Amazon delivery man in the elevator. He...</td>\n",
       "      <td>AtCustomerName We deliver Christmas trees and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>AtAmazonHelp I had pre-booked one plus 6t.init...</td>\n",
       "      <td>AtCustomerName I understand your concern regar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>AtAmazonHelp I just placed an order on that I ...</td>\n",
       "      <td>AtCustomerName Hey there, Fred. Could you plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>AtAmazonHelp AtAmazonHelp AtAmazonHelpIN @amit...</td>\n",
       "      <td>AtCustomerName I get your disappointment. We’d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>AtAmazonHelp I presume UK</td>\n",
       "      <td>AtCustomerName Did you get the chance to view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>My favourite recent discovery?   The error mes...</td>\n",
       "      <td>AtCustomerName It is Great !!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    inp  \\\n",
       "7465  AtAmazonHelp I cancelled my prime account 3 ti...   \n",
       "1332  @UPSHelp AtAmazonHelp I AM NOT PICKING THIS PA...   \n",
       "5700  AtAmazonHelp Can you please conclude so that I...   \n",
       "7090  AtAmazonHelp December the 3rd it still shows a...   \n",
       "7440  Met an Amazon delivery man in the elevator. He...   \n",
       "5280  AtAmazonHelp I had pre-booked one plus 6t.init...   \n",
       "6917  AtAmazonHelp I just placed an order on that I ...   \n",
       "914   AtAmazonHelp AtAmazonHelp AtAmazonHelpIN @amit...   \n",
       "310                           AtAmazonHelp I presume UK   \n",
       "507   My favourite recent discovery?   The error mes...   \n",
       "\n",
       "                                                   outp  \n",
       "7465  AtCustomerName My apologies for the unexpected...  \n",
       "1332  AtCustomerName I'm sorry to hear about this ex...  \n",
       "5700  AtCustomerName I get you're upset regarding yo...  \n",
       "7090  AtCustomerName I'm very sorry for the inconven...  \n",
       "7440  AtCustomerName We deliver Christmas trees and ...  \n",
       "5280  AtCustomerName I understand your concern regar...  \n",
       "6917  AtCustomerName Hey there, Fred. Could you plea...  \n",
       "914   AtCustomerName I get your disappointment. We’d...  \n",
       "310   AtCustomerName Did you get the chance to view ...  \n",
       "507                      AtCustomerName It is Great !!!  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.inp=lines.inp.apply(lambda x: x.lower())\n",
    "lines.outp=lines.outp.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the length as 50\n",
    "lines.inp=lines.inp.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines.outp=lines.outp.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.inp=lines.inp.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.outp=lines.outp.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_digits = str.maketrans('', '', digits)\n",
    "#lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "#lines.fr=lines.fr.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inp</th>\n",
       "      <th>outp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>atamazonhelp i’ve called you help support and ...</td>\n",
       "      <td>atcustomername wed like to make sure this is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>atamazonhelpecho hello COMMA can you please ad...</td>\n",
       "      <td>atcustomername thats odd wed like to live trou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>really atamazonhelp your delivery people take ...</td>\n",
       "      <td>atcustomername wed like to get this escalated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>atamazonhelp hi i ordered some lego as a prese...</td>\n",
       "      <td>atcustomername i do apologize for the way your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>atamazonhelp can you please tell me the custom...</td>\n",
       "      <td>atcustomername hey there we can be reached 247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>atamazonhelp atamazonhelp not happy i won’t ge...</td>\n",
       "      <td>atcustomername very sorry to hear your order i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>atamazonhelp plus this isnt the first time del...</td>\n",
       "      <td>atcustomername apologies  the seller should re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>atamazonhelp just tell me what to do when i fo...</td>\n",
       "      <td>atcustomername please try resetting your passw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7786</th>\n",
       "      <td>atamazonhelp  i really love your service COMMA...</td>\n",
       "      <td>atcustomername thanks for sharing your feedbac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>atamazonhelp im trying to avoid spending money...</td>\n",
       "      <td>atcustomername the release date for this item ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    inp  \\\n",
       "3868  atamazonhelp i’ve called you help support and ...   \n",
       "6508  atamazonhelpecho hello COMMA can you please ad...   \n",
       "4477  really atamazonhelp your delivery people take ...   \n",
       "4004  atamazonhelp hi i ordered some lego as a prese...   \n",
       "796   atamazonhelp can you please tell me the custom...   \n",
       "7050  atamazonhelp atamazonhelp not happy i won’t ge...   \n",
       "2722  atamazonhelp plus this isnt the first time del...   \n",
       "3324  atamazonhelp just tell me what to do when i fo...   \n",
       "7786  atamazonhelp  i really love your service COMMA...   \n",
       "2019  atamazonhelp im trying to avoid spending money...   \n",
       "\n",
       "                                                   outp  \n",
       "3868  atcustomername wed like to make sure this is t...  \n",
       "6508  atcustomername thats odd wed like to live trou...  \n",
       "4477  atcustomername wed like to get this escalated ...  \n",
       "4004  atcustomername i do apologize for the way your...  \n",
       "796   atcustomername hey there we can be reached 247...  \n",
       "7050  atcustomername very sorry to hear your order i...  \n",
       "2722  atcustomername apologies  the seller should re...  \n",
       "3324  atcustomername please try resetting your passw...  \n",
       "7786  atcustomername thanks for sharing your feedbac...  \n",
       "2019  atcustomername the release date for this item ...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.outp = lines.outp.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=set()\n",
    "for inp in lines.inp:\n",
    "    for word in inp.split():\n",
    "        if word not in all_words:\n",
    "            all_words.add(word)\n",
    "    \n",
    "all_outp_words=set()\n",
    "for outp in lines.outp:\n",
    "    for word in outp.split():\n",
    "        if word not in all_words:\n",
    "            all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15081"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list=[]\n",
    "for l in lines.inp:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_input_length = np.max(length_list)\n",
    "max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list=[]\n",
    "for l in lines.outp:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_output_length = np.max(length_list)\n",
    "max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(list(all_words))\n",
    "num_tokens = len(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(all_words)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8512922880"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines.outp)*max_input_length*num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(lines.inp[:MINI_BATCH_SIZE]), max_input_length),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(lines.outp[:MINI_BATCH_SIZE]), max_output_length),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(lines.outp[:MINI_BATCH_SIZE]), max_output_length, num_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines.outp[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines.inp[:MINI_BATCH_SIZE], lines.outp[:MINI_BATCH_SIZE])):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = token_index[word]\n",
    "\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build keras encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_tokens, embedding_size)(encoder_inputs)\n",
    "encoder = LSTM(max_input_length, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex=  Embedding(num_tokens, embedding_size)\n",
    "\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(max_input_length, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, None, 72)     1085832     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 72)     1085832     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 72), (None,  41760       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 72), ( 41760       embedding_10[0][0]               \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 15081)  1100913     lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,356,097\n",
      "Trainable params: 3,356,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/200\n",
      "950/950 [==============================] - 75s 79ms/step - loss: 4.4754 - acc: 0.0275 - val_loss: 4.1108 - val_acc: 0.0243\n",
      "Epoch 2/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 3.6945 - acc: 0.0252 - val_loss: 3.3360 - val_acc: 0.0197\n",
      "Epoch 3/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 3.0819 - acc: 0.0172 - val_loss: 2.9612 - val_acc: 0.0197\n",
      "Epoch 4/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.8355 - acc: 0.0172 - val_loss: 2.8142 - val_acc: 0.0197\n",
      "Epoch 5/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.7258 - acc: 0.0177 - val_loss: 2.7387 - val_acc: 0.0187\n",
      "Epoch 6/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.6677 - acc: 0.0214 - val_loss: 2.7030 - val_acc: 0.0200\n",
      "Epoch 7/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.6354 - acc: 0.0359 - val_loss: 2.6832 - val_acc: 0.0364\n",
      "Epoch 8/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.6159 - acc: 0.0373 - val_loss: 2.6699 - val_acc: 0.0354\n",
      "Epoch 9/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.6031 - acc: 0.0362 - val_loss: 2.6617 - val_acc: 0.0334\n",
      "Epoch 10/200\n",
      "950/950 [==============================] - 66s 69ms/step - loss: 2.5938 - acc: 0.0349 - val_loss: 2.6598 - val_acc: 0.0331\n",
      "Epoch 11/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.5861 - acc: 0.0347 - val_loss: 2.6560 - val_acc: 0.0331\n",
      "Epoch 12/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.5792 - acc: 0.0347 - val_loss: 2.6550 - val_acc: 0.0334\n",
      "Epoch 13/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.5719 - acc: 0.0345 - val_loss: 2.6461 - val_acc: 0.0334\n",
      "Epoch 14/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.5601 - acc: 0.0345 - val_loss: 2.6313 - val_acc: 0.0334\n",
      "Epoch 15/200\n",
      "950/950 [==============================] - 67s 71ms/step - loss: 2.5440 - acc: 0.0345 - val_loss: 2.6211 - val_acc: 0.0334\n",
      "Epoch 16/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.5300 - acc: 0.0346 - val_loss: 2.6086 - val_acc: 0.0334\n",
      "Epoch 17/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.5156 - acc: 0.0346 - val_loss: 2.5955 - val_acc: 0.0331\n",
      "Epoch 18/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.5012 - acc: 0.0344 - val_loss: 2.5837 - val_acc: 0.0351\n",
      "Epoch 19/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.4863 - acc: 0.0347 - val_loss: 2.5685 - val_acc: 0.0351\n",
      "Epoch 20/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.4706 - acc: 0.0363 - val_loss: 2.5558 - val_acc: 0.0420\n",
      "Epoch 21/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.4557 - acc: 0.0397 - val_loss: 2.5419 - val_acc: 0.0410\n",
      "Epoch 22/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.4389 - acc: 0.0399 - val_loss: 2.5298 - val_acc: 0.0393\n",
      "Epoch 23/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.4243 - acc: 0.0411 - val_loss: 2.5120 - val_acc: 0.0436\n",
      "Epoch 24/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.4088 - acc: 0.0417 - val_loss: 2.5013 - val_acc: 0.0423\n",
      "Epoch 25/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.3937 - acc: 0.0425 - val_loss: 2.4880 - val_acc: 0.0433\n",
      "Epoch 26/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.3786 - acc: 0.0425 - val_loss: 2.4747 - val_acc: 0.0452\n",
      "Epoch 27/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.3642 - acc: 0.0425 - val_loss: 2.4639 - val_acc: 0.0469\n",
      "Epoch 28/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.3488 - acc: 0.0434 - val_loss: 2.4492 - val_acc: 0.0456\n",
      "Epoch 29/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.3338 - acc: 0.0451 - val_loss: 2.4398 - val_acc: 0.0446\n",
      "Epoch 30/200\n",
      "950/950 [==============================] - 66s 69ms/step - loss: 2.3188 - acc: 0.0471 - val_loss: 2.4239 - val_acc: 0.0472\n",
      "Epoch 31/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.3036 - acc: 0.0482 - val_loss: 2.4183 - val_acc: 0.0511\n",
      "Epoch 32/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.2901 - acc: 0.0487 - val_loss: 2.3999 - val_acc: 0.0508\n",
      "Epoch 33/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 2.2756 - acc: 0.0494 - val_loss: 2.3882 - val_acc: 0.0525\n",
      "Epoch 34/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.2617 - acc: 0.0512 - val_loss: 2.3812 - val_acc: 0.0518\n",
      "Epoch 35/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.2468 - acc: 0.0553 - val_loss: 2.3674 - val_acc: 0.0544\n",
      "Epoch 36/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.2351 - acc: 0.0570 - val_loss: 2.3537 - val_acc: 0.0584\n",
      "Epoch 37/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.2204 - acc: 0.0598 - val_loss: 2.3422 - val_acc: 0.0597\n",
      "Epoch 38/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.2092 - acc: 0.0608 - val_loss: 2.3327 - val_acc: 0.0610\n",
      "Epoch 39/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.1977 - acc: 0.0622 - val_loss: 2.3236 - val_acc: 0.0616\n",
      "Epoch 40/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.1826 - acc: 0.0645 - val_loss: 2.3274 - val_acc: 0.0610\n",
      "Epoch 41/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.1702 - acc: 0.0652 - val_loss: 2.3069 - val_acc: 0.0636\n",
      "Epoch 42/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.1597 - acc: 0.0664 - val_loss: 2.2990 - val_acc: 0.0646\n",
      "Epoch 43/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 2.1478 - acc: 0.0680 - val_loss: 2.2898 - val_acc: 0.0652\n",
      "Epoch 44/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.1343 - acc: 0.0688 - val_loss: 2.2782 - val_acc: 0.0689\n",
      "Epoch 45/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.1239 - acc: 0.0705 - val_loss: 2.2706 - val_acc: 0.0679\n",
      "Epoch 46/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.1127 - acc: 0.0715 - val_loss: 2.2594 - val_acc: 0.0725\n",
      "Epoch 47/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.0983 - acc: 0.0743 - val_loss: 2.2515 - val_acc: 0.0731\n",
      "Epoch 48/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.0881 - acc: 0.0747 - val_loss: 2.2441 - val_acc: 0.0738\n",
      "Epoch 49/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.0769 - acc: 0.0773 - val_loss: 2.2405 - val_acc: 0.0738\n",
      "Epoch 50/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.0638 - acc: 0.0783 - val_loss: 2.2301 - val_acc: 0.0767\n",
      "Epoch 51/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.0525 - acc: 0.0805 - val_loss: 2.2239 - val_acc: 0.0764\n",
      "Epoch 52/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 2.0413 - acc: 0.0813 - val_loss: 2.2102 - val_acc: 0.0784\n",
      "Epoch 53/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.0299 - acc: 0.0823 - val_loss: 2.2004 - val_acc: 0.0790\n",
      "Epoch 54/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 2.0160 - acc: 0.0844 - val_loss: 2.1922 - val_acc: 0.0810\n",
      "Epoch 55/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 2.0066 - acc: 0.0857 - val_loss: 2.1826 - val_acc: 0.0813\n",
      "Epoch 56/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.9919 - acc: 0.0880 - val_loss: 2.1682 - val_acc: 0.0803\n",
      "Epoch 57/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.9795 - acc: 0.0898 - val_loss: 2.1631 - val_acc: 0.0820\n",
      "Epoch 58/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.9668 - acc: 0.0912 - val_loss: 2.1508 - val_acc: 0.0862\n",
      "Epoch 59/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.9547 - acc: 0.0933 - val_loss: 2.1412 - val_acc: 0.0882\n",
      "Epoch 60/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.9419 - acc: 0.0958 - val_loss: 2.1328 - val_acc: 0.0925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.9280 - acc: 0.0986 - val_loss: 2.1240 - val_acc: 0.0934\n",
      "Epoch 62/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.9165 - acc: 0.1008 - val_loss: 2.1104 - val_acc: 0.0951\n",
      "Epoch 63/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.9028 - acc: 0.1038 - val_loss: 2.1014 - val_acc: 0.0984\n",
      "Epoch 64/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.8888 - acc: 0.1067 - val_loss: 2.0929 - val_acc: 0.1000\n",
      "Epoch 65/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.8775 - acc: 0.1088 - val_loss: 2.0809 - val_acc: 0.1000\n",
      "Epoch 66/200\n",
      "950/950 [==============================] - 65s 69ms/step - loss: 1.8622 - acc: 0.1114 - val_loss: 2.0711 - val_acc: 0.1023\n",
      "Epoch 67/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 1.8493 - acc: 0.1149 - val_loss: 2.0561 - val_acc: 0.1036\n",
      "Epoch 68/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.8376 - acc: 0.1168 - val_loss: 2.0492 - val_acc: 0.1075\n",
      "Epoch 69/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.8229 - acc: 0.1198 - val_loss: 2.0446 - val_acc: 0.1072\n",
      "Epoch 70/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.8100 - acc: 0.1221 - val_loss: 2.0412 - val_acc: 0.1105\n",
      "Epoch 71/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.7983 - acc: 0.1244 - val_loss: 2.0192 - val_acc: 0.1141\n",
      "Epoch 72/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.7850 - acc: 0.1271 - val_loss: 2.0018 - val_acc: 0.1164\n",
      "Epoch 73/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.7709 - acc: 0.1300 - val_loss: 1.9957 - val_acc: 0.1170\n",
      "Epoch 74/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.7598 - acc: 0.1316 - val_loss: 1.9853 - val_acc: 0.1197\n",
      "Epoch 75/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.7443 - acc: 0.1348 - val_loss: 1.9668 - val_acc: 0.1213\n",
      "Epoch 76/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.7344 - acc: 0.1368 - val_loss: 1.9559 - val_acc: 0.1246\n",
      "Epoch 77/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.7182 - acc: 0.1395 - val_loss: 1.9501 - val_acc: 0.1272\n",
      "Epoch 78/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.7097 - acc: 0.1412 - val_loss: 1.9348 - val_acc: 0.1338\n",
      "Epoch 79/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.6960 - acc: 0.1445 - val_loss: 1.9274 - val_acc: 0.1331\n",
      "Epoch 80/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.6830 - acc: 0.1461 - val_loss: 1.9175 - val_acc: 0.1374\n",
      "Epoch 81/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.6708 - acc: 0.1482 - val_loss: 1.9057 - val_acc: 0.1416\n",
      "Epoch 82/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.6612 - acc: 0.1501 - val_loss: 1.8978 - val_acc: 0.1416\n",
      "Epoch 83/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.6468 - acc: 0.1524 - val_loss: 1.8874 - val_acc: 0.1430\n",
      "Epoch 84/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.6345 - acc: 0.1537 - val_loss: 1.8793 - val_acc: 0.1420\n",
      "Epoch 85/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.6266 - acc: 0.1558 - val_loss: 1.8712 - val_acc: 0.1449\n",
      "Epoch 86/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.6125 - acc: 0.1568 - val_loss: 1.8767 - val_acc: 0.1426\n",
      "Epoch 87/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.6010 - acc: 0.1588 - val_loss: 1.8497 - val_acc: 0.1475\n",
      "Epoch 88/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.5925 - acc: 0.1604 - val_loss: 1.8425 - val_acc: 0.1538\n",
      "Epoch 89/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5795 - acc: 0.1630 - val_loss: 1.8378 - val_acc: 0.1534\n",
      "Epoch 90/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5707 - acc: 0.1643 - val_loss: 1.8286 - val_acc: 0.1557\n",
      "Epoch 91/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5590 - acc: 0.1665 - val_loss: 1.8204 - val_acc: 0.1567\n",
      "Epoch 92/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5487 - acc: 0.1683 - val_loss: 1.8092 - val_acc: 0.1587\n",
      "Epoch 93/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5391 - acc: 0.1699 - val_loss: 1.8011 - val_acc: 0.1620\n",
      "Epoch 94/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5283 - acc: 0.1714 - val_loss: 1.8035 - val_acc: 0.1593\n",
      "Epoch 95/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5181 - acc: 0.1731 - val_loss: 1.7935 - val_acc: 0.1616\n",
      "Epoch 96/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.5108 - acc: 0.1741 - val_loss: 1.7780 - val_acc: 0.1675\n",
      "Epoch 97/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4978 - acc: 0.1754 - val_loss: 1.7821 - val_acc: 0.1649\n",
      "Epoch 98/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4906 - acc: 0.1777 - val_loss: 1.7685 - val_acc: 0.1639\n",
      "Epoch 99/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4797 - acc: 0.1789 - val_loss: 1.7610 - val_acc: 0.1692\n",
      "Epoch 100/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4727 - acc: 0.1798 - val_loss: 1.7510 - val_acc: 0.1731\n",
      "Epoch 101/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4609 - acc: 0.1820 - val_loss: 1.7561 - val_acc: 0.1705\n",
      "Epoch 102/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4531 - acc: 0.1824 - val_loss: 1.7409 - val_acc: 0.1754\n",
      "Epoch 103/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4440 - acc: 0.1833 - val_loss: 1.7335 - val_acc: 0.1744\n",
      "Epoch 104/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4351 - acc: 0.1844 - val_loss: 1.7431 - val_acc: 0.1702\n",
      "Epoch 105/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4278 - acc: 0.1860 - val_loss: 1.7294 - val_acc: 0.1744\n",
      "Epoch 106/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4168 - acc: 0.1876 - val_loss: 1.7300 - val_acc: 0.1734\n",
      "Epoch 107/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4097 - acc: 0.1880 - val_loss: 1.7255 - val_acc: 0.1731\n",
      "Epoch 108/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.4000 - acc: 0.1901 - val_loss: 1.7198 - val_acc: 0.1757\n",
      "Epoch 109/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3942 - acc: 0.1912 - val_loss: 1.7047 - val_acc: 0.1787\n",
      "Epoch 110/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3844 - acc: 0.1918 - val_loss: 1.6953 - val_acc: 0.1816\n",
      "Epoch 111/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3768 - acc: 0.1932 - val_loss: 1.7002 - val_acc: 0.1797\n",
      "Epoch 112/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3697 - acc: 0.1944 - val_loss: 1.7051 - val_acc: 0.1780\n",
      "Epoch 113/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3618 - acc: 0.1952 - val_loss: 1.6914 - val_acc: 0.1807\n",
      "Epoch 114/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3531 - acc: 0.1966 - val_loss: 1.6813 - val_acc: 0.1846\n",
      "Epoch 115/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.3456 - acc: 0.1972 - val_loss: 1.6856 - val_acc: 0.1823\n",
      "Epoch 116/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.3399 - acc: 0.1984 - val_loss: 1.6785 - val_acc: 0.1843\n",
      "Epoch 117/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.3298 - acc: 0.1997 - val_loss: 1.6767 - val_acc: 0.1816\n",
      "Epoch 118/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3244 - acc: 0.2008 - val_loss: 1.6669 - val_acc: 0.1862\n",
      "Epoch 119/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3169 - acc: 0.2015 - val_loss: 1.6665 - val_acc: 0.1849\n",
      "Epoch 120/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3111 - acc: 0.2018 - val_loss: 1.6636 - val_acc: 0.1866\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950/950 [==============================] - 63s 66ms/step - loss: 1.3038 - acc: 0.2033 - val_loss: 1.6591 - val_acc: 0.1862\n",
      "Epoch 122/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2958 - acc: 0.2043 - val_loss: 1.6615 - val_acc: 0.1872\n",
      "Epoch 123/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.2904 - acc: 0.2052 - val_loss: 1.6518 - val_acc: 0.1892\n",
      "Epoch 124/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2818 - acc: 0.2066 - val_loss: 1.6536 - val_acc: 0.1895\n",
      "Epoch 125/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2764 - acc: 0.2067 - val_loss: 1.6480 - val_acc: 0.1911\n",
      "Epoch 126/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2703 - acc: 0.2075 - val_loss: 1.6447 - val_acc: 0.1902\n",
      "Epoch 127/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2626 - acc: 0.2094 - val_loss: 1.6376 - val_acc: 0.1918\n",
      "Epoch 128/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 1.2565 - acc: 0.2105 - val_loss: 1.6455 - val_acc: 0.1918\n",
      "Epoch 129/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2499 - acc: 0.2111 - val_loss: 1.6408 - val_acc: 0.1944\n",
      "Epoch 130/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2434 - acc: 0.2111 - val_loss: 1.6372 - val_acc: 0.1957\n",
      "Epoch 131/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.2360 - acc: 0.2136 - val_loss: 1.6247 - val_acc: 0.1961\n",
      "Epoch 132/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2322 - acc: 0.2134 - val_loss: 1.6251 - val_acc: 0.1977\n",
      "Epoch 133/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.2235 - acc: 0.2146 - val_loss: 1.6282 - val_acc: 0.1961\n",
      "Epoch 134/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.2198 - acc: 0.2156 - val_loss: 1.6177 - val_acc: 0.2007\n",
      "Epoch 135/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.2125 - acc: 0.2162 - val_loss: 1.6135 - val_acc: 0.2010\n",
      "Epoch 136/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.2065 - acc: 0.2167 - val_loss: 1.6292 - val_acc: 0.1987\n",
      "Epoch 137/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.2009 - acc: 0.2180 - val_loss: 1.6145 - val_acc: 0.2000\n",
      "Epoch 138/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1947 - acc: 0.2190 - val_loss: 1.6092 - val_acc: 0.1997\n",
      "Epoch 139/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1909 - acc: 0.2192 - val_loss: 1.6060 - val_acc: 0.1997\n",
      "Epoch 140/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1823 - acc: 0.2205 - val_loss: 1.6031 - val_acc: 0.2013\n",
      "Epoch 141/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.1779 - acc: 0.2213 - val_loss: 1.6035 - val_acc: 0.2000\n",
      "Epoch 142/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.1713 - acc: 0.2221 - val_loss: 1.6020 - val_acc: 0.2016\n",
      "Epoch 143/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1674 - acc: 0.2228 - val_loss: 1.5986 - val_acc: 0.2003\n",
      "Epoch 144/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.1585 - acc: 0.2237 - val_loss: 1.6040 - val_acc: 0.1997\n",
      "Epoch 145/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1560 - acc: 0.2244 - val_loss: 1.5983 - val_acc: 0.2013\n",
      "Epoch 146/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.1502 - acc: 0.2250 - val_loss: 1.5905 - val_acc: 0.2033\n",
      "Epoch 147/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1433 - acc: 0.2268 - val_loss: 1.5873 - val_acc: 0.2036\n",
      "Epoch 148/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.1380 - acc: 0.2265 - val_loss: 1.5842 - val_acc: 0.2020\n",
      "Epoch 149/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.1355 - acc: 0.2273 - val_loss: 1.5844 - val_acc: 0.2036\n",
      "Epoch 150/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1296 - acc: 0.2287 - val_loss: 1.5813 - val_acc: 0.2030\n",
      "Epoch 151/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.1229 - acc: 0.2289 - val_loss: 1.5834 - val_acc: 0.2020\n",
      "Epoch 152/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.1168 - acc: 0.2297 - val_loss: 1.5909 - val_acc: 0.2036\n",
      "Epoch 153/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.1138 - acc: 0.2300 - val_loss: 1.5809 - val_acc: 0.2026\n",
      "Epoch 154/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.1088 - acc: 0.2309 - val_loss: 1.5790 - val_acc: 0.2043\n",
      "Epoch 155/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.1036 - acc: 0.2316 - val_loss: 1.5718 - val_acc: 0.2059\n",
      "Epoch 156/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0968 - acc: 0.2333 - val_loss: 1.5763 - val_acc: 0.2023\n",
      "Epoch 157/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.0932 - acc: 0.2329 - val_loss: 1.5696 - val_acc: 0.2066\n",
      "Epoch 158/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0881 - acc: 0.2338 - val_loss: 1.5685 - val_acc: 0.2026\n",
      "Epoch 159/200\n",
      "950/950 [==============================] - 64s 68ms/step - loss: 1.0838 - acc: 0.2349 - val_loss: 1.5718 - val_acc: 0.2036\n",
      "Epoch 160/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0767 - acc: 0.2364 - val_loss: 1.5763 - val_acc: 0.2016\n",
      "Epoch 161/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0756 - acc: 0.2353 - val_loss: 1.5683 - val_acc: 0.2033\n",
      "Epoch 162/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0681 - acc: 0.2376 - val_loss: 1.5815 - val_acc: 0.2036\n",
      "Epoch 163/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0631 - acc: 0.2378 - val_loss: 1.5665 - val_acc: 0.2039\n",
      "Epoch 164/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0587 - acc: 0.2379 - val_loss: 1.5657 - val_acc: 0.2056\n",
      "Epoch 165/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0550 - acc: 0.2396 - val_loss: 1.5617 - val_acc: 0.2052\n",
      "Epoch 166/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0492 - acc: 0.2398 - val_loss: 1.5611 - val_acc: 0.2059\n",
      "Epoch 167/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0447 - acc: 0.2410 - val_loss: 1.5585 - val_acc: 0.2052\n",
      "Epoch 168/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0399 - acc: 0.2422 - val_loss: 1.5616 - val_acc: 0.2036\n",
      "Epoch 169/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0359 - acc: 0.2417 - val_loss: 1.5626 - val_acc: 0.2030\n",
      "Epoch 170/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0318 - acc: 0.2429 - val_loss: 1.5628 - val_acc: 0.2036\n",
      "Epoch 171/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.0268 - acc: 0.2437 - val_loss: 1.5575 - val_acc: 0.2075\n",
      "Epoch 172/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0216 - acc: 0.2445 - val_loss: 1.5753 - val_acc: 0.2033\n",
      "Epoch 173/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 1.0196 - acc: 0.2443 - val_loss: 1.5592 - val_acc: 0.2062\n",
      "Epoch 174/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0120 - acc: 0.2457 - val_loss: 1.5604 - val_acc: 0.2085\n",
      "Epoch 175/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 1.0093 - acc: 0.2460 - val_loss: 1.5743 - val_acc: 0.2066\n",
      "Epoch 176/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0040 - acc: 0.2471 - val_loss: 1.5508 - val_acc: 0.2095\n",
      "Epoch 177/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 1.0004 - acc: 0.2475 - val_loss: 1.5569 - val_acc: 0.2085\n",
      "Epoch 178/200\n",
      "950/950 [==============================] - 65s 68ms/step - loss: 0.9976 - acc: 0.2486 - val_loss: 1.5544 - val_acc: 0.2079\n",
      "Epoch 179/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9918 - acc: 0.2492 - val_loss: 1.5540 - val_acc: 0.2111\n",
      "Epoch 180/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9882 - acc: 0.2495 - val_loss: 1.5539 - val_acc: 0.2108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 0.9828 - acc: 0.2505 - val_loss: 1.5506 - val_acc: 0.2075\n",
      "Epoch 182/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9811 - acc: 0.2512 - val_loss: 1.5696 - val_acc: 0.2052\n",
      "Epoch 183/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9744 - acc: 0.2523 - val_loss: 1.5624 - val_acc: 0.2069\n",
      "Epoch 184/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9718 - acc: 0.2529 - val_loss: 1.5561 - val_acc: 0.2043\n",
      "Epoch 185/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9669 - acc: 0.2539 - val_loss: 1.5560 - val_acc: 0.2069\n",
      "Epoch 186/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9621 - acc: 0.2538 - val_loss: 1.5569 - val_acc: 0.2069\n",
      "Epoch 187/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9615 - acc: 0.2537 - val_loss: 1.5601 - val_acc: 0.2098\n",
      "Epoch 188/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 0.9550 - acc: 0.2557 - val_loss: 1.5487 - val_acc: 0.2118\n",
      "Epoch 189/200\n",
      "950/950 [==============================] - 66s 69ms/step - loss: 0.9507 - acc: 0.2562 - val_loss: 1.5508 - val_acc: 0.2138\n",
      "Epoch 190/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9478 - acc: 0.2566 - val_loss: 1.5516 - val_acc: 0.2102\n",
      "Epoch 191/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 0.9434 - acc: 0.2576 - val_loss: 1.5458 - val_acc: 0.2082\n",
      "Epoch 192/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9388 - acc: 0.2581 - val_loss: 1.5522 - val_acc: 0.2111\n",
      "Epoch 193/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9364 - acc: 0.2591 - val_loss: 1.5453 - val_acc: 0.2095\n",
      "Epoch 194/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9321 - acc: 0.2602 - val_loss: 1.5534 - val_acc: 0.2079\n",
      "Epoch 195/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9294 - acc: 0.2600 - val_loss: 1.5537 - val_acc: 0.2079\n",
      "Epoch 196/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 0.9230 - acc: 0.2617 - val_loss: 1.5570 - val_acc: 0.2111\n",
      "Epoch 197/200\n",
      "950/950 [==============================] - 66s 70ms/step - loss: 0.9191 - acc: 0.2623 - val_loss: 1.5531 - val_acc: 0.2069\n",
      "Epoch 198/200\n",
      "950/950 [==============================] - 63s 67ms/step - loss: 0.9183 - acc: 0.2625 - val_loss: 1.5543 - val_acc: 0.2141\n",
      "Epoch 199/200\n",
      "950/950 [==============================] - 63s 66ms/step - loss: 0.9109 - acc: 0.2639 - val_loss: 1.5433 - val_acc: 0.2125\n",
      "Epoch 200/200\n",
      "950/950 [==============================] - 64s 67ms/step - loss: 0.9101 - acc: 0.2639 - val_loss: 1.5431 - val_acc: 0.2164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4bfd160518>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=200,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 72)          1085832   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                [(None, 72), (None, 72),  41760     \n",
      "=================================================================\n",
      "Total params: 1,127,592\n",
      "Trainable params: 1,127,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rich/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_15:0' shape=(?, 72) dtype=float32>, <tf.Tensor 'input_16:0' shape=(?, 72) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 72)     1085832     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 72), ( 41760       embedding_10[1][0]               \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 15081)  1100913     lstm_10[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,228,505\n",
      "Trainable params: 2,228,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(max_input_length,))\n",
    "decoder_state_input_c = Input(shape=(max_input_length,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "\n",
    "with open('encoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(encoder_model.to_json())\n",
    "encoder_model.save_weights('encoder_model_weights.h5')\n",
    "\n",
    "with open('decoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(decoder_model.to_json())\n",
    "decoder_model.save_weights('decoder_model_weights.h5')\n",
    "\n",
    "\n",
    "decoder_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE MODE\n",
    "#### Create sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 72)          1085832   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                [(None, 72), (None, 72),  41760     \n",
      "=================================================================\n",
      "Total params: 1,127,592\n",
      "Trainable params: 1,127,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 72)     1085832     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 72), ( 41760       embedding_10[0][0]               \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 15081)  1100913     lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,228,505\n",
      "Trainable params: 2,228,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TODO:  Load Models from disk\n",
    "from keras.models import model_from_json\n",
    "def load_model(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "encoder_model = load_model('encoder_model.json', 'encoder_model_weights.h5')\n",
    "decoder_model = load_model('decoder_model.json', 'decoder_model_weights.h5')\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_char_index = dict(\n",
    "    (i, char) for char, i in token_index.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Function to generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_char_index[sampled_token_index]\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 180):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the some translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: atamazonhelp hi COMMA i ordered something on the 25th october and it’s still not arrived i’ve messaged the seller and it said they can’t receive my email i do not think this company is genuine how do i get a refund thanks\n",
      "Decoded sentence:  atcustomername your order order will be able to take a closer look at this further appreciate your understanding pv _END\n",
      "Decoded sentence length: 21\n",
      "-\n",
      "Input sentence: atamazonhelp hi there COMMA how long after returning damaged books to you via doddle on 201118 COMMA would i expect to see a change in my orders status and or refund it currently says return started  thank you jacq\n",
      "Decoded sentence:  atcustomername your order order will be able to take a closer look at this further appreciate your understanding pv _END\n",
      "Decoded sentence length: 21\n",
      "-\n",
      "Input sentence: atamazonhelp for you guys 12hour means  COMMA COMMA COMMA how many days  COMMA yesterday you guys told me that our back end reach you with in 12hr but now i completed 15 hours no one contact me \n",
      "Decoded sentence:  atcustomername your order order will be able to take a closer look at this further appreciate your understanding pv _END\n",
      "Decoded sentence length: 21\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [100, 200, 300]:\n",
    "    input_seq = encoder_input_data[seq_index]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', lines.inp[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence.replace(' COMMA', ','))\n",
    "    print('Decoded sentence length: {}'.format(len(decoded_sentence.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.zeros(72)\n",
    "input_text = 'atamazonhelp where is my order i have been waiting for weeks to receive it and it still has not arrived'\n",
    "for i, word in enumerate(input_text.split()):\n",
    "        input[i] = token_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3387e+04, 1.3592e+04, 1.4329e+04, 8.7110e+03, 1.2226e+04,\n",
       "       1.4295e+04, 5.7590e+03, 1.0188e+04, 1.4198e+04, 4.7390e+03,\n",
       "       5.7060e+03, 1.1700e+03, 6.8560e+03, 9.9320e+03, 9.7300e+03,\n",
       "       9.9320e+03, 8.1840e+03, 1.1000e+01, 2.5580e+03, 2.2500e+02,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = decode_sequence(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' atcustomername your order order will be able to take a closer look at this further appreciate your understanding pv _END'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # python 3.x\n",
    "    import pickle\n",
    "\n",
    "with open('token_index.p', 'wb') as fp:\n",
    "    pickle.dump(token_index, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
